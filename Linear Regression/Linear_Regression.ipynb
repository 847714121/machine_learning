{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "这个 notebook 是线性回归的代码实现，提供梯度下降和 Normal Equation 两种方法进行模型的训练。在模型优化中，提供 batch gradient descent、mini-batch gradient descent 和 stochastic gradient descent 三种方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 假设函数 $h$ 的实现\n",
    "输入 $X$ 是一个 $m$ 行 $n$ 列的矩阵，每一行是一个样本的 $n$ 个属性的参数。$\\boldsymbol \\theta$ 是我们待求的参数，是一个 $n$ 维向量，假设函数的计算公式如下：\n",
    "$$\n",
    "h(X) =  X  \\boldsymbol \\theta\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数功能：计算假设函数的结果，公式如上\n",
    "#     输入：X (m, n) 是一个 m 行 n 列的矩阵，m 是样本个数，n 是 theta 的维度\n",
    "#           parameters 是一个 (n, 1) 的 n 维向量，其就是公式中的 theta\n",
    "#     输出：y (m, 1) 是一个 m 维的列向量，每一行是一个样本的输出\n",
    "def hypothese(X, parameters):\n",
    "    y = np.dot(X, parameters)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#函数功能：将训练集数据按照 miniBatchSize 划分成多个小批数据，默认为 64\n",
    "#    输入：X 输入数据集，(m, n)\n",
    "#          y 输出数据集，(m, 1)\n",
    "#          miniBatchSize，划分后每个小批数据的个数，默认为 64\n",
    "#          seed 随机数种子，默认为 0\n",
    "#    输出：miniBatchs 划分好的数据集，放在一个列表中\n",
    "def randomMiniBatchs(X, y, miniBatchSize = 64, seed = 0):\n",
    "    m = X.shape[0]             #取得样本个数 m\n",
    "    np.random.seed(seed)       #设置随机数种子\n",
    "    miniBatchs = []            #存放分类好的小批数据集\n",
    "    miniBatch_X = []           #临时存放一个重排序后的小批的输入数据集\n",
    "    miniBatch_y = []           #临时存放一个重排序后的小批的标签数据集\n",
    "    miniBatch = ()             #临时存放一个批\n",
    "    \n",
    "    permutation = list(np.random.permutation(m))     #将索引随机重排列\n",
    "    \n",
    "    numCompleteBatchs = math.floor(m / miniBatchSize)   #存放完整的批的个数\n",
    "    for i in range(0, numCompleteBatchs):\n",
    "        miniBatch_X = X[permutation[i * miniBatchSize : (i + 1) * miniBatchSize] ,:]\n",
    "        miniBatch_y = y[permutation[i * miniBatchSize : (i + 1) * miniBatchSize] ,:]\n",
    "        miniBatch = (miniBatch_X, miniBatch_y)     #将 miniBatch_X 和 miniBatch_y 打包\n",
    "        miniBatchs.append(miniBatch)               #将这一批数据放入 miniBatchs 中\n",
    "    \n",
    "    if(m % miniBatchSize != 0):\n",
    "        miniBatch_X = X[permutation[numCompleteBatchs * miniBatchSize : ] ,:]   #将剩下的数据组成一个批\n",
    "        miniBatch_y = y[permutation[numCompleteBatchs * miniBatchSize : ] ,:]   \n",
    "        miniBatch = (miniBatch_X, miniBatch_y)\n",
    "        miniBatchs.append(miniBatch)\n",
    "    \n",
    "    return miniBatchs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#函数功能：在数据前增加一列或一行全为1\n",
    "#    输入：X_train 原始数据 (m, n)\n",
    "#          ax = 1表示在最左边加1列， = 0 表示在最上面加一行\n",
    "#    输出：X 加完后的数组  (m + 1, n) 或 (m, n + 1)\n",
    "def expendOneCol(X_train, ax = 1):\n",
    "    X = np.insert(X_train, 0, values = np.ones((1,X_train.shape[(ax+1)%2])), axis = ax)\n",
    "    return X;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#函数功能：初始化参数\n",
    "#    输入：m 维度\n",
    "#    输出：parameters 初始化为 0 的数组 (m, 1)\n",
    "def initialParameters(m):\n",
    "    parameters = np.zeros((m, 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#函数功能：计算梯度\n",
    "#    输入：X_train 训练数据\n",
    "#          y 标签数据\n",
    "#          h 假设函数结果\n",
    "#    输出：grad 梯度\n",
    "def calculateGrad(X_train, y, h):\n",
    "    grad = np.dot(X_train.T, h - y)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#函数功能：计算损失值\n",
    "#    输入：y 标签\n",
    "#          h 假设函数结果\n",
    "#    输出：J 损失值\n",
    "def calLossFunction(y, h):\n",
    "    sub = (h - y)\n",
    "    J = np.dot(sub.T, sub) / 2\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#函数功能：线性回归训练，可调 batchSize 进行 SGD、Mini batch GD 或 BGD\n",
    "#    输入：X_train 是一个 m 行 n 列矩阵，是训练数据\n",
    "#          y_train 是 X_train 对应的标签，维度是 (m, 1)\n",
    "#          parameters 是训练参数，维度是 (m, 1)\n",
    "#          learningRate 是学习率\n",
    "#          miniBatchSize 是每次更新的批样本数、当 miniBatchSize = 1 时进行的是随机梯度下降、当 miniBatchSize = 其他值进行的是 mini-batch GD\n",
    "#          iteration 是迭代循环更新的次数\n",
    "#    输出：训练好的参数 parameters\n",
    "def linearRegressionTrain(X_train, y_train, parameters, learningRate, iteration, miniBatchSize = 64, seed = 0):\n",
    "    miniBatchs = []    #用于存放随机重组的小批数据\n",
    "    miniBatch_X = []   #用于临时存放一批数据的输入\n",
    "    miniBatch_y = []   #用来临时存放一批数据的标签\n",
    "    \n",
    "    miniBatchs = randomMiniBatchs(X_train, y_train, miniBatchSize, seed)   #将数据分批\n",
    "    \n",
    "    for itera in range(0, iteration):\n",
    "        for batchNum in range(0, len(miniBatchs)):\n",
    "            (miniBatch_X, miniBatch_y) = miniBatchs[batchNum]   #取出第 batchNum 个批的数据\n",
    "            h = hypothese(miniBatch_X, parameters)              #计算这一批数据的假设函数的结果\n",
    "            grad = np.dot(miniBatch_X.T, h - miniBatch_y)       #计算梯度\n",
    "            parameters = parameters - learningRate * grad       #更新参数\n",
    "            \n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, y):\n",
    "    X_train = expendOneCol(X_train)\n",
    "    (m, n) = X_train.shape\n",
    "    \n",
    "    parameters = initialParameters(n)\n",
    "    \n",
    "    parameters = linearRegressionTrain(X_train, y, parameters, 0.01, 10)\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd   \n",
    "import numpy as np\n",
    "\n",
    "data_X = pd.read_csv('./r_train_X.csv', encoding = 'big5')  #r_train_X.csv文件要与python代码文件(.py或.ipynb)在同一个目录下才行\n",
    "data_y = pd.read_csv('./r_train_y.csv', encoding = 'big5')  #r_train_y.csv文件要与python代码文件(.py或.ipynb)在同一个目录下才行\n",
    "X = data_X.to_numpy()  #将数据转换成numpy数据\n",
    "y = data_y.to_numpy()  #将数据转换成numpy数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
