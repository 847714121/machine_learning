{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from linear_model import load_binary_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_label, dev_data, dev_label = load_binary_dataset()\n",
    "\n",
    "print(\"train data:  \" + str(train_data.shape))\n",
    "print(\"train label: \" + str(train_label.shape))\n",
    "print(\"test data:   \" + str(dev_data.shape))\n",
    "print(\"test label:  \" + str(dev_label.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#先取 100 个出来测试代码\n",
    "train_data = train_data[0:1000]\n",
    "train_label = train_label[0:1000]\n",
    "\n",
    "print(\"train data:  \" + str(train_data.shape))\n",
    "print(\"train label: \" + str(train_label.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    计算输入的向量的 sigmoid 函数结果\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(X, train = True, X_mean = None, X_std = None):\n",
    "    \"\"\"\n",
    "    X_mean 为 None 时表示传入的是训练集，进行归一化并返回 X_mean 和 X_std\n",
    "    X_mean 不为 None 时表示传入的是测试集，进行归一化\n",
    "    \"\"\"\n",
    "    if(train):\n",
    "        flag = 0\n",
    "        X_mean = np.mean(X, 0).reshape(1, -1)\n",
    "        X_std  = np.std(X, 0).reshape(1,-1)\n",
    "        \n",
    "    X = (X - X_mean) / (X_std + 1e-8)\n",
    "    \n",
    "    if(train):\n",
    "        return X, X_mean, X_std\n",
    "    else:\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothese(X, parameters):\n",
    "    return sigmoid(np.dot(X, parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_gred(X, sub):\n",
    "    gred = np.dot(X.T, sub)\n",
    "    return gred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_loss(y, h):\n",
    "    loss = - np.dot(y.T, np.log(h + 1e-8)) - np.dot((1-y).T, np.log(1 - h + 1e-8))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(X, Y):\n",
    "    randomize = np.arange(X.shape[0])\n",
    "    np.random.shuffle(randomize)\n",
    "    return (X[randomize], Y[randomize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_miniBatch(X, Y, batchSize = 64):\n",
    "    X_shuffled, Y_shuffled = shuffle(X, Y)    #对样本进行随机打乱\n",
    "    m, n = X.shape                            #获取样本个数\n",
    "    completeBatchNum = int(np.ceil(m / batchSize)) #计算完整的 batch 个数\n",
    "    miniBatchs = []\n",
    "    \n",
    "    for i in range(completeBatchNum):\n",
    "        tempBatch_X = X[i * batchSize : (i + 1) * batchSize]\n",
    "        tempBatch_Y = Y[i * batchSize : (i + 1) * batchSize]\n",
    "        miniBatchs.append((tempBatch_X, tempBatch_Y))\n",
    "    \n",
    "    if(m % batchSize != 0):\n",
    "        tempBatch_X = X[completeBatchNum * batchSize :]\n",
    "        tempBatch_Y = Y[completeBatchNum * batchSize :]\n",
    "        miniBatchs.append((tempBatch_X, tempBatch_Y))\n",
    "    \n",
    "    return miniBatchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_train(X, y, parameters, iteration = 1000, learning_rate = 0.0003, train_method = \"batch\", batchSize = 64):\n",
    "    m, n = X.shape\n",
    "    \n",
    "    if(train_method == \"miniBatch\"):\n",
    "        miniBatchs = split_to_miniBatch(X, y, batchSize)\n",
    "        batchNum = len(miniBatchs)\n",
    "    \n",
    "    if (train_method == \"miniBatch\"):\n",
    "        subIteration = batchNum\n",
    "    elif (train_method == \"stochastic\"):\n",
    "        subIteration = m\n",
    "    else:\n",
    "        subIteration = 1\n",
    "    \n",
    "    loss_history = np.zeros((iteration * subIteration,1))\n",
    "    \n",
    "    for ite in range(iteration):\n",
    "        for subIte in range(subIteration):\n",
    "            if (train_method == \"miniBatch\"):    #取出当前循环需要用于计算的 batch\n",
    "                batch_X, batch_y = miniBatchs[subIte]\n",
    "            elif (train_method == \"stochastic\"):\n",
    "                batch_X, batch_y = (X[subIte, : ].reshape(1 , -1), y[subIte].reshape(-1 , 1))\n",
    "            else:\n",
    "                batch_X, batch_y = (X, y)\n",
    "                    \n",
    "            h = hypothese(batch_X, parameters)\n",
    "            sub = h - batch_y\n",
    "            gred = cal_gred(batch_X, sub)\n",
    "            \n",
    "            #X_h = hypothese(X, parameters)\n",
    "            #loss_history[ite] = cal_loss(y, X_h)\n",
    "            if((ite * subIteration + subIte) % 100 == 0):\n",
    "                X_h = hypothese(X, parameters)\n",
    "                loss_history[ite * subIteration + subIte] = cal_loss(y, X_h)\n",
    "                print(\"iteration \"+ str(ite * subIteration + subIte) +\" loss: \" +  str(loss_history[ite * subIteration + subIte]))\n",
    "            parameters = parameters - learning_rate * gred\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(h):\n",
    "    # This function returns a truth value prediction for each row of X \n",
    "    # by rounding the result of logistic regression function.\n",
    "    return np.round(h).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(Y_pred, Y_label):\n",
    "    # This function calculates prediction accuracy\n",
    "    acc = 1 - np.mean(np.abs(Y_pred - Y_label))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_col(X):\n",
    "    m, n = X.shape\n",
    "    ones = np.ones((m, 1))\n",
    "    return np.hstack((ones, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_mean, train_std = normalization(train_data)\n",
    "#dev_data = normalization(dev_data, False, train_mean, train_std)\n",
    "train_data = add_col(train_data)\n",
    "m, n = train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = np.zeros((n, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = logistic_regression_train(train_data, train_label, parameters, iteration = 1000, learning_rate = 0.00001, train_method = \"batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练集上的正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = hypothese(train_data, parameters)\n",
    "p = predict(h)\n",
    "acc = accuracy(p, train_label)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 验证集上的正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = normalization(dev_data, False, train_mean, train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = add_col(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = hypothese(dev_data, parameters)\n",
    "p = predict(h)\n",
    "acc = accuracy(p, dev_label)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
