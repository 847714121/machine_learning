{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from linear_model import load_binary_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_label, dev_data, dev_label = load_binary_dataset()\n",
    "\n",
    "print(\"train data:  \" + str(train_data.shape))\n",
    "print(\"train label: \" + str(train_label.shape))\n",
    "print(\"test data:   \" + str(dev_data.shape))\n",
    "print(\"test label:  \" + str(dev_label.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#先取 100 个出来测试代码\n",
    "train_data = train_data[0:1000]\n",
    "train_label = train_label[0:1000]\n",
    "\n",
    "print(\"train data:  \" + str(train_data.shape))\n",
    "print(\"train label: \" + str(train_label.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    计算输入的向量的 sigmoid 函数结果\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(X, train = True, X_mean = None, X_std = None):\n",
    "    \"\"\"\n",
    "    X_mean 为 None 时表示传入的是训练集，进行归一化并返回 X_mean 和 X_std\n",
    "    X_mean 不为 None 时表示传入的是测试集，进行归一化\n",
    "    \"\"\"\n",
    "    if(train):\n",
    "        flag = 0\n",
    "        X_mean = np.mean(X, 0).reshape(1, -1)\n",
    "        X_std  = np.std(X, 0).reshape(1,-1)\n",
    "        \n",
    "    X = (X - X_mean) / (X_std + 1e-8)\n",
    "    \n",
    "    if(train):\n",
    "        return X, X_mean, X_std\n",
    "    else:\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothese(X, parameters):\n",
    "    return sigmoid(np.dot(X, parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_gred(X, sub):\n",
    "    gred = np.dot(X.T, sub)\n",
    "    return gred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_loss(y, h):\n",
    "    loss = - np.dot(y.T, np.log(h + 1e-8)) - np.dot((1-y).T, np.log(1 - h + 1e-8))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_train(X, y, parameters, iteration = 10000, learning_rate = 0.0003):\n",
    "    m, n = X.shape\n",
    "    loss_history = np.zeros((iteration,1))\n",
    "    \n",
    "    for ite in range(iteration):\n",
    "        h = hypothese(X, parameters)\n",
    "        sub = h - y\n",
    "        gred = cal_gred(X, sub)\n",
    "        loss_history[ite] = cal_loss(y, h)\n",
    "        if(ite % 200 == 0):\n",
    "            print(\"iteration \"+ str(ite) +\" loss: \" +  str(loss_history[ite]))\n",
    "        parameters = parameters - learning_rate * gred\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(h):\n",
    "    # This function returns a truth value prediction for each row of X \n",
    "    # by rounding the result of logistic regression function.\n",
    "    return np.round(h).astype(np.int)\n",
    "    \n",
    "def accuracy(Y_pred, Y_label):\n",
    "    # This function calculates prediction accuracy\n",
    "    acc = 1 - np.mean(np.abs(Y_pred - Y_label))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_mean, train_std = normalization(train_data)\n",
    "#dev_data = normalization(dev_data, False, train_mean, train_std)\n",
    "m, n = train_data.shape\n",
    "\n",
    "parameters = np.zeros((n, 1))\n",
    "parameters = logistic_regression_train(train_data, train_label, parameters, iteration = 10000, learning_rate = 0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = hypothese(train_data, parameters)\n",
    "p = predict(h)\n",
    "acc = accuracy(p, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
